{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-ranker_for_ClariQ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d48517fab1f4826a99b5053f71ccf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd8edeb69bca4a6aa6aa644cd3c896d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e66069b612964f37b897276d35fb341b",
              "IPY_MODEL_5e0c3ce7cd944b92afc1ec5fd5c21773"
            ]
          }
        },
        "cd8edeb69bca4a6aa6aa644cd3c896d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e66069b612964f37b897276d35fb341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f88dcb06d364fcb9be11a7980b10775",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d07b3f2a9094d64867f893667c253bf"
          }
        },
        "5e0c3ce7cd944b92afc1ec5fd5c21773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a3f5ed58d9d4d308643184b11cef78a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.24MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2996a43d436a4d7aae36c7d44626af4a"
          }
        },
        "9f88dcb06d364fcb9be11a7980b10775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d07b3f2a9094d64867f893667c253bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a3f5ed58d9d4d308643184b11cef78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2996a43d436a4d7aae36c7d44626af4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "921b8a33cba14558b84c9c1916e47e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5690414a8bd544aca30f2fd97a3cf905",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_979b0281fd4e49df865b0c184e8d1bb6",
              "IPY_MODEL_f21f758f54e8481c973853989e87d200"
            ]
          }
        },
        "5690414a8bd544aca30f2fd97a3cf905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "979b0281fd4e49df865b0c184e8d1bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ade0cde134a84b63bedde4facd44f428",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_251e3d3acb5b49e7ba596286f5b367ab"
          }
        },
        "f21f758f54e8481c973853989e87d200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db8f058e9381429686f007982fcc577d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.15kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a05fb8ec4b844c69b0d8adc1b659168f"
          }
        },
        "ade0cde134a84b63bedde4facd44f428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "251e3d3acb5b49e7ba596286f5b367ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db8f058e9381429686f007982fcc577d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a05fb8ec4b844c69b0d8adc1b659168f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc067504beca4713b3a5796551a99414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_02932572abae45d0b615af4c68af473e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94d38c0feeff4b768ab3188ec4440f35",
              "IPY_MODEL_a7653c51b7b44cb783278ee3fd4f5d59"
            ]
          }
        },
        "02932572abae45d0b615af4c68af473e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94d38c0feeff4b768ab3188ec4440f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1fe00e2fd6f4d36b844ec1bdc39d6a1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_131daad9d4bb457c84510b5326ed4cf5"
          }
        },
        "a7653c51b7b44cb783278ee3fd4f5d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e289eaabf4964d50824ed92df65b9744",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [08:37&lt;00:00, 851kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5ed1b54d7ef464c831572fc2ef22523"
          }
        },
        "c1fe00e2fd6f4d36b844ec1bdc39d6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "131daad9d4bb457c84510b5326ed4cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e289eaabf4964d50824ed92df65b9744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5ed1b54d7ef464c831572fc2ef22523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5cB8x9R7Lp",
        "colab_type": "text"
      },
      "source": [
        "## Installing transformer-rankers and dependencies\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndWAB7rQRg0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/Guzpenha/transformer_rankers.git\n",
        "!wget https://raw.githubusercontent.com/Guzpenha/transformer_rankers/master/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AfQvDVjLJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install Anserini which is also a requirement for part of transformer-rankers (BM25 Negative Samplers)\n",
        "!apt-get install maven -qq\n",
        "!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
        "!cd anserini; mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true\n",
        "!ls anserini/target/appassembler/bin/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oa0JWLcSAeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Google colab with torch 1.5 doesnt see the GPU\n",
        "!pip install -I torch==1.4.0\n",
        "import torch\n",
        "torch.cuda.get_device_name(0)  #This should ouptut a GPU device name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG57GzJWSOPY",
        "colab_type": "text"
      },
      "source": [
        "## Downloading ClariQ data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv2XbhwqSP7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/clariq\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/dev.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/train.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/question_bank.tsv\n",
        "!mv data/clariq/train.tsv data/clariq/train_original.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQIZq0XSljz",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess ClariQ for transformer-rankers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAtrf1lSoP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data_path = \"./data/\"\n",
        "\n",
        "train = pd.read_csv(data_path+\"clariq/train_original.tsv\", sep=\"\\t\")\n",
        "valid = pd.read_csv(data_path+\"clariq/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "train = train[[\"initial_request\", \"question\"]]\n",
        "train.columns = [\"query\", \"clarifying_question\"]\n",
        "train = train[~train[\"clarifying_question\"].isnull()]\n",
        "\n",
        "valid = valid[[\"initial_request\", \"question\"]]\n",
        "valid.columns = [\"query\", \"clarifying_question\"]\n",
        "valid = valid[~valid[\"clarifying_question\"].isnull()]\n",
        "\n",
        "train.to_csv(data_path+\"clariq/train.tsv\", sep=\"\\t\", index=False)\n",
        "valid.to_csv(data_path+\"clariq/valid.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAEKgEf6TNzz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b3ba1a81-d269-4803-84a7-eb995246bcdb"
      },
      "source": [
        "# For transformer-rankers we only need a pandas DF with query (here the initial request) \n",
        "# and relevant documents (here the clarifying questions).\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>clarifying_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you interested in seeing barack obamas family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know barack obamas geneology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know about obamas ancestors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know who is currently alive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you looking for biological information on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              query                                clarifying_question\n",
              "0  Tell me about Obama family tree.  are you interested in seeing barack obamas family\n",
              "1  Tell me about Obama family tree.     would you like to know barack obamas geneology\n",
              "2  Tell me about Obama family tree.      would you like to know about obamas ancestors\n",
              "3  Tell me about Obama family tree.  would you like to know who is currently alive ...\n",
              "4  Tell me about Obama family tree.  are you looking for biological information on ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOeCaSHOVDLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bc93f129-719b-4ede-fe17-4ae59cf2e89a"
      },
      "source": [
        "# We will sample negative samples for training using the question bank\n",
        "question_bank = pd.read_csv(data_path+\"clariq/question_bank.tsv\", sep=\"\\t\")\n",
        "question_bank.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q00001</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q00002</td>\n",
              "      <td>a total cholesterol of 180 to 200 mgdl 10 to 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q00003</td>\n",
              "      <td>about how many years experience do you want th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q00004</td>\n",
              "      <td>according to anima the bible or what other source</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q00005</td>\n",
              "      <td>ae you looking for examples of septic system d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  question_id                                           question\n",
              "0      Q00001                                                NaN\n",
              "1      Q00002  a total cholesterol of 180 to 200 mgdl 10 to 1...\n",
              "2      Q00003  about how many years experience do you want th...\n",
              "3      Q00004  according to anima the bible or what other source\n",
              "4      Q00005  ae you looking for examples of septic system d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAILVs8MUL9k",
        "colab_type": "text"
      },
      "source": [
        "## Training a transformer-ranker for ClariQ (RQ2)\n",
        "\n",
        "The problem is to retrieve the most relevant clarifying question for a given query. We will train a BERT-ranker using transformer-rankers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC9XL8YvTXoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1d48517fab1f4826a99b5053f71ccf5c",
            "cd8edeb69bca4a6aa6aa644cd3c896d6",
            "e66069b612964f37b897276d35fb341b",
            "5e0c3ce7cd944b92afc1ec5fd5c21773",
            "9f88dcb06d364fcb9be11a7980b10775",
            "9d07b3f2a9094d64867f893667c253bf",
            "0a3f5ed58d9d4d308643184b11cef78a",
            "2996a43d436a4d7aae36c7d44626af4a",
            "921b8a33cba14558b84c9c1916e47e50",
            "5690414a8bd544aca30f2fd97a3cf905",
            "979b0281fd4e49df865b0c184e8d1bb6",
            "f21f758f54e8481c973853989e87d200",
            "ade0cde134a84b63bedde4facd44f428",
            "251e3d3acb5b49e7ba596286f5b367ab",
            "db8f058e9381429686f007982fcc577d",
            "a05fb8ec4b844c69b0d8adc1b659168f",
            "bc067504beca4713b3a5796551a99414",
            "02932572abae45d0b615af4c68af473e",
            "94d38c0feeff4b768ab3188ec4440f35",
            "a7653c51b7b44cb783278ee3fd4f5d59",
            "c1fe00e2fd6f4d36b844ec1bdc39d6a1",
            "131daad9d4bb457c84510b5326ed4cf5",
            "e289eaabf4964d50824ed92df65b9744",
            "e5ed1b54d7ef464c831572fc2ef22523"
          ]
        },
        "outputId": "f42190e1-5b2b-4894-aa05-bd077c3a3d6c"
      },
      "source": [
        "from transformer_rankers.trainers import transformer_trainer\n",
        "from transformer_rankers.datasets import dataset\n",
        "from transformer_rankers.negative_samplers import negative_sampling\n",
        "from transformer_rankers.eval import results_analyses_tools\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "logging.basicConfig(\n",
        "  level=logging.INFO,\n",
        "  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "  handlers=[\n",
        "      logging.StreamHandler(sys.stdout)\n",
        "  ]\n",
        ")\n",
        "\n",
        "#The combination of query and question are not that big.\n",
        "max_seq_len = 50\n",
        "\n",
        "#Lets use an almost balanced amount of positive and negative samples during training.\n",
        "average_relevant_per_query = train.groupby(\"query\").count().mean().values[0]\n",
        "\n",
        "#Instantiate BM25 negative sampler.\n",
        "ns_train = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query) , \n",
        "                    \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "ns_val = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query), \n",
        "                    \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "\n",
        "# We could also use random sampling which does not require Anserini.\n",
        "# ns_train = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "# ns_val = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "\n",
        "#Create the loaders for the dataset, with the respective negative samplers\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataloader = dataset.QueryDocumentDataLoader(train_df=train,\n",
        "                    val_df=valid, test_df=valid,\n",
        "                    tokenizer=tokenizer, negative_sampler_train=ns_train,\n",
        "                    negative_sampler_val=ns_val, task_type='classification',\n",
        "                    train_batch_size=12, val_batch_size=12, max_seq_len=max_seq_len,\n",
        "                    sample_data=-1, cache_path=\"./data/clariq/\")\n",
        "\n",
        "train_loader, val_loader, test_loader = dataloader.\\\n",
        "  get_pytorch_dataloaders()\n",
        "\n",
        "#Use BERT (any model that has SequenceClassification class from HuggingFace would work here)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Instantiate trainer that handles fitting.\n",
        "trainer = transformer_trainer.TransformerTrainer(model=model,\n",
        "  train_loader=train_loader,\n",
        "  val_loader=val_loader, test_loader=test_loader,\n",
        "  num_ns_eval=int(average_relevant_per_query), task_type=\"classification\", tokenizer=tokenizer,\n",
        "  validate_every_epochs=1, num_validation_instances=-1,\n",
        "  num_epochs=1, lr=5e-7, sacred_ex=None)\n",
        "\n",
        "#Train (our validation eval uses the NS sampling procedure)\n",
        "trainer.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:23,222 [INFO] Lock 139694630808936 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2020-08-07 17:03:23,224 [INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpc1wij842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d48517fab1f4826a99b5053f71ccf5c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:03:23,520 [INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,522 [INFO] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,527 [INFO] Lock 139694630808936 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2020-08-07 17:03:23,529 [INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,577 [INFO] Train instances per batch 12\n",
            "2020-08-07 17:03:23,638 [INFO] Generating instances with signature set_train_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:01<00:00, 97.69it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:25,562 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:33,169 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:33,383 [INFO] Set train Instance 0 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,384 [INFO] Set train Instance 0 document \n",
            "\n",
            "are you interested in indiana child support\n",
            "\n",
            "2020-08-07 17:03:33,385 [INFO] Set train Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2024, 2017, 4699, 1999, 5242, 2775, 2490, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,388 [INFO] Set train Instance 1 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,389 [INFO] Set train Instance 1 document \n",
            "\n",
            "which counties law would you like clarification on\n",
            "\n",
            "2020-08-07 17:03:33,391 [INFO] Set train Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2029, 5721, 2375, 2052, 2017, 2066, 18856, 8486, 10803, 2006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,393 [INFO] Set train Instance 2 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,394 [INFO] Set train Instance 2 document \n",
            "\n",
            "would you like a list of forms for processing child support claims\n",
            "\n",
            "2020-08-07 17:03:33,396 [INFO] Set train Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2052, 2017, 2066, 1037, 2862, 1997, 3596, 2005, 6364, 2775, 2490, 4447, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,516 [INFO] Total of 16981 instances were cached.\n",
            "2020-08-07 17:03:33,538 [INFO] Generating instances with signature set_val_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 150.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:33,881 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:35,882 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:35,915 [INFO] Set val Instance 0 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,916 [INFO] Set val Instance 0 document \n",
            "\n",
            "are you interested in brooks brothers clearance shirts\n",
            "\n",
            "2020-08-07 17:03:35,919 [INFO] Set val Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2024, 2017, 4699, 1999, 8379, 3428, 14860, 11344, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,922 [INFO] Set val Instance 1 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,924 [INFO] Set val Instance 1 document \n",
            "\n",
            "do you want to know about brooks brothers clearance center in garland nc\n",
            "\n",
            "2020-08-07 17:03:35,926 [INFO] Set val Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 2113, 2055, 8379, 3428, 14860, 2415, 1999, 17017, 13316, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,927 [INFO] Set val Instance 2 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,929 [INFO] Set val Instance 2 document \n",
            "\n",
            "do you want to shop online for clothes\n",
            "\n",
            "2020-08-07 17:03:35,930 [INFO] Set val Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 4497, 3784, 2005, 4253, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,961 [INFO] Total of 4411 instances were cached.\n",
            "2020-08-07 17:03:35,982 [INFO] Generating instances with signature set_test_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 142.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:36,342 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:38,301 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:38,479 [INFO] Set test Instance 0 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,480 [INFO] Set test Instance 0 document \n",
            "\n",
            "are you interested in brooks brothers clearance shirts\n",
            "\n",
            "2020-08-07 17:03:38,483 [INFO] Set test Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2024, 2017, 4699, 1999, 8379, 3428, 14860, 11344, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,485 [INFO] Set test Instance 1 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,487 [INFO] Set test Instance 1 document \n",
            "\n",
            "do you want to know about brooks brothers clearance center in garland nc\n",
            "\n",
            "2020-08-07 17:03:38,488 [INFO] Set test Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 2113, 2055, 8379, 3428, 14860, 2415, 1999, 17017, 13316, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,490 [INFO] Set test Instance 2 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,491 [INFO] Set test Instance 2 document \n",
            "\n",
            "do you want to shop online for clothes\n",
            "\n",
            "2020-08-07 17:03:38,493 [INFO] Set test Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 4497, 3784, 2005, 4253, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,530 [INFO] Total of 4411 instances were cached.\n",
            "2020-08-07 17:03:38,776 [INFO] Lock 139694630845128 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "2020-08-07 17:03:38,780 [INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmptkm5vfhr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "921b8a33cba14558b84c9c1916e47e50",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:03:38,986 [INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,988 [INFO] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,990 [INFO] Lock 139694630845128 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "2020-08-07 17:03:38,992 [INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,994 [INFO] Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2020-08-07 17:03:39,217 [INFO] Lock 139694572620656 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "2020-08-07 17:03:39,219 [INFO] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplnt80y73\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc067504beca4713b3a5796551a99414",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:04:21,308 [INFO] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:21,310 [INFO] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:21,312 [INFO] Lock 139694572620656 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "2020-08-07 17:04:21,313 [INFO] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:24,718 [INFO] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "2020-08-07 17:04:24,719 [INFO] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-08-07 17:04:24,727 [INFO] Device cuda\n",
            "2020-08-07 17:04:24,728 [INFO] Num GPU 1\n",
            "2020-08-07 17:04:29,615 [INFO] Total batches per epoch : 1416\n",
            "2020-08-07 17:04:29,616 [INFO] Validating every 1 epoch.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1416/1416 [07:12<00:00,  3.28it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:27<00:00, 13.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-07 17:12:09,138 [INFO] Epoch 1 val nDCG@10 0.951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0fLLW7wV3YE",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating with ClariQ evaluation scripts\n",
        "The above code uses the transformer-ranker's built-in evaluation. This means that we are only ranking from a set of K (int(average_relevant_per_query) in the example) candidate questions, a re-ranking scenario where all the positive examples are in the candidate list. RQ2 of ClariQ requires us to rank from the entire question_bank. Additionally it evaluates whether the clarifying questions helps for document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4BSJT7vVT01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "a067d813-6093-44a0-fe1f-6a941984422f"
      },
      "source": [
        "! git clone https://github.com/aliannejadi/ClariQ.git ClariQ-repo\n",
        "! pip install rank_bm25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ClariQ-repo'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 120 (delta 56), reused 77 (delta 33), pack-reused 12\u001b[K\n",
            "Receiving objects: 100% (120/120), 24.07 MiB | 30.13 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rguln-2txb5o",
        "colab_type": "text"
      },
      "source": [
        "### Re-rank BM25 with the fine-tuned BERT-ranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnToD3YFAAXK",
        "colab_type": "text"
      },
      "source": [
        "Lets first use the bm25 [example](https://colab.research.google.com/drive/1g_Sc9j5fYT1hiOxif6BVH5NHNt-icxtT#scrollTo=_7_2LTXoXqK7) from Mohammad to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HgNLYQJ1PaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rerank_top_k = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39S7b3jvAG5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "367c4037-c065-495d-d0e2-6eea330ddae0"
      },
      "source": [
        "# Imports required packages, defines stem & tokenizez function\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def stem_tokenize(text, remove_stopwords=True):\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
        "                                      for word in nltk.word_tokenize(sent)]\n",
        "  tokens = [word for word in tokens if word not in \\\n",
        "          nltk.corpus.stopwords.words('english')]\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Files paths\n",
        "request_file_path = './ClariQ-repo/data/dev.tsv'\n",
        "question_bank_path = './ClariQ-repo/data/question_bank.tsv'\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_bm25'\n",
        "\n",
        "# Reads files and build bm25 corpus (index)\n",
        "dev = pd.read_csv(request_file_path, sep='\\t')\n",
        "question_bank = pd.read_csv(question_bank_path, sep='\\t').fillna('')\n",
        "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
        "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))\n",
        "bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
        "bm25 = BM25Okapi(bm25_corpus)\n",
        "\n",
        "# Runs bm25 for every query and stores output in file.\n",
        "examples = []\n",
        "all_preds_bm25 = []\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), \n",
        "                                    bm25_corpus, \n",
        "                                    n=rerank_top_k)\n",
        "    bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
        "    docs = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question'].tolist()\n",
        "    preds = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question_id'].tolist()\n",
        "    all_preds_bm25.append(preds)\n",
        "    for doc in docs[:rerank_top_k]:\n",
        "      examples.append((query, doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EELGiVqv6_6L",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to transform this dataset in the format required for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHBszQljAT_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.data.data_collator import DefaultDataCollator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformer_rankers.utils import utils\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)\n",
        "collator = DefaultDataCollator()\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collator.collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXPpw7FTE2x2",
        "colab_type": "text"
      },
      "source": [
        "Now we can run the trained model on this dataset and  save the predictions to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVGhE-yQAvH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4f12be0-a01d-46b2-d9e3-49db114e15eb"
      },
      "source": [
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], rerank_top_k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:10<00:00,  9.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDAJuVEAwZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-reranker'\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    document_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_k_scores_idx = (-document_scores).argsort()[:rerank_top_k]  \n",
        "    preds = np.array(all_preds_bm25[tid_idx])[top_k_scores_idx]\n",
        "    for i, qid in enumerate(preds):\n",
        "      fo.write('{} 0 {} {} {} BERT-reranker\\n'.format(tid, qid, i, len(preds)-i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy9GH-rDCAyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ddce776d-e41f-49db-cb83-f21abcc1f5f1"
      },
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall5: 0.3474806038474769\n",
            "Recall10: 0.6136149763549145\n",
            "Recall20: 0.6912818698329535\n",
            "Recall30: 0.6912818698329535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEiMf2pqB0XC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ec6e89ac-a363-48c8-e43c-2ac5bcc9098b"
      },
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDCG1: 0.18958333333333333\n",
            "NDCG3: 0.17431329825264302\n",
            "NDCG5: 0.16796281956102732\n",
            "NDCG10: 0.1658691210524936\n",
            "NDCG20: 0.1527795302714777\n",
            "P1: 0.2375\n",
            "P3: 0.20416666666666666\n",
            "P5: 0.19\n",
            "P10: 0.176875\n",
            "P20: 0.1384375\n",
            "MRR100: 0.33301824879980596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBFowAVmtQH4",
        "colab_type": "text"
      },
      "source": [
        "### Full retrieval\n",
        "So let's first generate a dataset containing all combinations of dev queries \n",
        "and question_bank questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB2VwOPlc1ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.data.data_collator import DefaultDataCollator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "#Lets not use the null document for no question.\n",
        "all_documents = list(question_bank[\"question\"].values[1:])\n",
        "examples = []\n",
        "for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    for doc in all_documents:\n",
        "      examples.append((query, doc))\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)\n",
        "collator = DefaultDataCollator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddUbNbzGt73A",
        "colab_type": "text"
      },
      "source": [
        "Now we have to make the predictions and acumulate the logits by the number of candidate documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLF1qpmRpBZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98355783-92e9-4fb4-dc66-ec3a0d7d734c"
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collator.collate_batch)\n",
        "from transformer_rankers.utils import utils\n",
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], len(all_documents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12313/12313 [21:48<00:00,  9.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QefshgrStfja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-ranker'\n",
        "all_doc_ids = np.array(question_bank[\"question_id\"].values[1:])\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    all_documents_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_30_scores_idx = (-all_documents_scores).argsort()[:30]  \n",
        "    preds = all_doc_ids[top_30_scores_idx]\n",
        "    for i, qid in enumerate(preds):    \n",
        "      fo.write('{} 0 {} {} {} BERT-ranker\\n'.format(tid, qid, i, len(preds)-i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NlCqE7XwSh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "697a5b5f-e175-4d2b-f168-645186abacc2"
      },
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall5: 0.35055278656671846\n",
            "Recall10: 0.6154512724117988\n",
            "Recall20: 0.7253078340648\n",
            "Recall30: 0.7529370626793227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Znqwz6ZwUXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d302545a-6c0f-4fc3-84e2-e165b5afe5a7"
      },
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDCG1: 0.18958333333333333\n",
            "NDCG3: 0.17431329825264302\n",
            "NDCG5: 0.16796281956102732\n",
            "NDCG10: 0.1658691210524936\n",
            "NDCG20: 0.1527795302714777\n",
            "P1: 0.2375\n",
            "P3: 0.20416666666666666\n",
            "P5: 0.19\n",
            "P10: 0.176875\n",
            "P20: 0.1384375\n",
            "MRR100: 0.33301824879980596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKArX9JZBRUI",
        "colab_type": "text"
      },
      "source": [
        "## Results comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDPJh2oX_i4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "24aeeafd-83ff-4a13-e2f3-91b4e6f2721a"
      },
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}_question_relevance.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "cols = res_df.columns.tolist()\n",
        "res_df.sort_values([(\"value\",\"Recall10\")])[cols[-1:] + cols[:-1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>Recall5</th>\n",
              "      <th>Recall10</th>\n",
              "      <th>Recall20</th>\n",
              "      <th>Recall30</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.3246</td>\n",
              "      <td>0.5638</td>\n",
              "      <td>0.6675</td>\n",
              "      <td>0.6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-reranker</th>\n",
              "      <td>0.3475</td>\n",
              "      <td>0.6136</td>\n",
              "      <td>0.6913</td>\n",
              "      <td>0.6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-ranker</th>\n",
              "      <td>0.3506</td>\n",
              "      <td>0.6155</td>\n",
              "      <td>0.7253</td>\n",
              "      <td>0.7529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                value                           \n",
              "metric        Recall5 Recall10 Recall20 Recall30\n",
              "model                                           \n",
              "bm25           0.3246   0.5638   0.6675   0.6913\n",
              "BERT-reranker  0.3475   0.6136   0.6913   0.6913\n",
              "BERT-ranker    0.3506   0.6155   0.7253   0.7529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4SF6f7RDptU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6029df47-4213-46ba-b1bc-cff38ed49909"
      },
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "res_df.sort_values((\"value\", \"MRR100\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"11\" halign=\"left\">value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>MRR100</th>\n",
              "      <th>NDCG1</th>\n",
              "      <th>NDCG10</th>\n",
              "      <th>NDCG20</th>\n",
              "      <th>NDCG3</th>\n",
              "      <th>NDCG5</th>\n",
              "      <th>P1</th>\n",
              "      <th>P10</th>\n",
              "      <th>P20</th>\n",
              "      <th>P3</th>\n",
              "      <th>P5</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.3096</td>\n",
              "      <td>0.1859</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.1285</td>\n",
              "      <td>0.1608</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>0.1406</td>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-ranker</th>\n",
              "      <td>0.3330</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.1659</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.1743</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.1384</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-reranker</th>\n",
              "      <td>0.3330</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.1659</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.1743</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.1384</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                value                          ...                               \n",
              "metric         MRR100   NDCG1  NDCG10  NDCG20  ...     P10     P20      P3     P5\n",
              "model                                          ...                               \n",
              "bm25           0.3096  0.1859  0.1363  0.1285  ...  0.1406  0.1181  0.1896  0.175\n",
              "BERT-ranker    0.3330  0.1896  0.1659  0.1528  ...  0.1769  0.1384  0.2042  0.190\n",
              "BERT-reranker  0.3330  0.1896  0.1659  0.1528  ...  0.1769  0.1384  0.2042  0.190\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nlZ6KRfCS-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}