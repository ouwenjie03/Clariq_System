{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def stem_tokenize(text, remove_stopwords=True):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
    "                                      for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word for word in tokens if word not in \\\n",
    "          nltk.corpus.stopwords.words('english')]\n",
    "#     tokens = [word for word in tokens if not re.search(r'[^A-Za-z0-9]', word)]\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filepath):\n",
    "    topic_df = pd.read_csv(filepath, sep='\\t')\n",
    "    topic_df = topic_df.drop_duplicates(['topic_id', 'question_id']).reset_index(drop=True)\n",
    "    topic_df.fillna('', inplace=True)\n",
    "    return topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_df('../data/train.tsv')\n",
    "dev_df = load_df('../data/dev.tsv')\n",
    "question_bank = pd.read_csv('../data/question_bank.tsv', sep='\\t').dropna()\n",
    "all_df = pd.concat([train_df, dev_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed index\n",
    "word2question = {}\n",
    "for q in question_bank['question'].values:\n",
    "    for c in stem_tokenize(q):\n",
    "        if c not in word2question:\n",
    "            word2question[c] = []\n",
    "        word2question[c].append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_question_infos(topic_df):\n",
    "    tmp = topic_df[['topic_id', 'question']].groupby(by=['question']).count()\n",
    "    question2cnt = {tid: cnt for tid, cnt in zip(tmp.index, tmp['topic_id']) if cnt > 1}\n",
    "    return sorted(question2cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "[('', 159), ('are you looking for a specific web site', 18), ('are you looking for a specific web page', 10), ('are you interested in a specific web page', 4), ('are you looking for a specific website', 2), ('do you want a map', 2), ('do you want a phone number to call', 2), ('do you want the address', 2), ('do you want to know the address', 2), ('do you want to know their hours of operation', 2), ('is there any specific web page on your mind', 2), ('is this an emergency', 2), ('what is your price range', 2)]\n"
     ]
    }
   ],
   "source": [
    "train_infos = see_question_infos(train_df)\n",
    "print(len(train_df['topic_id'].unique()))\n",
    "print(train_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[('', 39), ('are you looking for a specific web page', 3), ('are you looking for a specific web site', 3), ('would you like to buy a book about this topic', 2)]\n"
     ]
    }
   ],
   "source": [
    "dev_infos = see_question_infos(dev_df)\n",
    "print(len(dev_df['topic_id'].unique()))\n",
    "print(dev_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 198), ('are you looking for a specific web site', 21), ('are you looking for a specific web page', 13), ('are you interested in a specific web page', 4), ('would you like to buy a book about this topic', 3), ('are you looking for a specific movie', 2), ('are you looking for a specific website', 2), ('are you looking for any specific photos', 2), ('are you referring to a software', 2), ('do you want a map', 2), ('do you want a phone number to call', 2), ('do you want the address', 2), ('do you want to know the address', 2), ('do you want to know their hours of operation', 2), ('is there any specific web page on your mind', 2), ('is this an emergency', 2), ('what is your price range', 2)]\n"
     ]
    }
   ],
   "source": [
    "all_infos = see_question_infos(pd.concat([train_df, dev_df]))\n",
    "print(all_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev in train\n",
    "\n",
    "devq2cnt = {}\n",
    "trainq = set([_q for _q in train_df['question']])\n",
    "for t, q in dev_df[['initial_request', 'question']].values:\n",
    "    if q in trainq:\n",
    "        if q not in devq2cnt:\n",
    "            devq2cnt[q] = [0, []]\n",
    "        devq2cnt[q][0] += 1\n",
    "        devq2cnt[q][1].append(t)\n",
    "sorted(devq2cnt.items(), key=lambda x: x[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words = ['software', 'movie', 'web', 'page', 'website', 'webpage', 'phone', 'photo', 'book', 'address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_questions = []\n",
    "for w in normal_words:\n",
    "    c = stem_tokenize(w)[0]\n",
    "    normal_questions.extend(word2question.get(c, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_questions.extend([_q for _q, _c in all_infos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_questions = set(normal_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_qids = set(question_bank[question_bank['question'].isin(normal_questions)]['question_id'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads files and build bm25 corpus (index)\n",
    "question_bank = pd.read_csv('../data/question_bank.tsv', sep='\\t').fillna('')\n",
    "\n",
    "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
    "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = []\n",
    "added_cnames = ['initial_request', 'answer', 'topic_desc']\n",
    "for qid in question_bank['question_id'].values:\n",
    "    words = []\n",
    "    for cname in added_cnames:\n",
    "        irs = train_df[train_df['question_id'] == qid][cname].unique()\n",
    "#         irs = all_df[all_df['question_id'] == qid][cname].unique()\n",
    "        for ir in irs:\n",
    "            ws = stem_tokenize(ir)\n",
    "            words.extend(ws)\n",
    "    words = list(set(words))\n",
    "    added_tokens.append(words)\n",
    "question_bank['tokens_from_train'] = added_tokens\n",
    "question_bank['all_tokens'] = question_bank['tokenized_question_list'] + question_bank['tokens_from_train']\n",
    "question_bank['all_token_str'] = question_bank['all_tokens'].map(lambda x: ' '.join(x))\n",
    "\n",
    "# add train_df initial_request tokens\n",
    "# bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
    "bm25_corpus = question_bank['all_tokens'].tolist()\n",
    "bm25 = BM25Okapi(bm25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_corpus_onlyq = question_bank['tokenized_question_list'].tolist()\n",
    "bm25_onlyq = BM25Okapi(bm25_corpus_onlyq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_bank['tokens_from_train_len'] = question_bank['tokens_from_train'].apply(len)\n",
    "notin_train = question_bank[question_bank['tokens_from_train_len'] == 0]['question_id'].values.tolist()\n",
    "notin_train = sorted(notin_train, key=lambda x: len(question_bank[question_bank['question_id'] == x]['tokenized_question_str'].values[0]))\n",
    "len(notin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3941"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notin_train = question_bank['question_id'].values.tolist()\n",
    "notin_train = sorted(notin_train, key=lambda x: len(question_bank[question_bank['question_id'] == x]['tokenized_question_str'].values[0]))\n",
    "len(notin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test normal_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_recall_results(dev, ir_id_columns='topic_id', ir_columns='initial_request', topn=-1):\n",
    "    all_preds = {}\n",
    "    for tid in dev[ir_id_columns].unique():\n",
    "        query = dev.loc[dev[ir_id_columns]==tid, ir_columns].tolist()[0]\n",
    "        if 'clarification_need' in dev.columns:\n",
    "            cneed = dev.loc[dev[ir_id_columns]==tid, 'clarification_need'].tolist()[0]\n",
    "        else:\n",
    "            cneed = 0\n",
    "        bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), bm25_corpus, n=100)\n",
    "        bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
    "        preds = question_bank.set_index('all_token_str').loc[bm25_q_list, 'question_id'].tolist()\n",
    "        \n",
    "        bm25_ranked_list_onlyq = bm25_onlyq.get_top_n(stem_tokenize(query, True), bm25_corpus_onlyq, n=100)\n",
    "        bm25_q_list_onlyq = [' '.join(sent) for sent in bm25_ranked_list_onlyq]\n",
    "        preds_onlyq = question_bank.set_index('tokenized_question_str').loc[bm25_q_list_onlyq, 'question_id'].tolist()\n",
    "        \n",
    "        insert_qs = [_q for _q in preds if _q in preds_onlyq]\n",
    "#         print(tid, len(preds), len(preds_onlyq), len(insert_qs))\n",
    "        new_preds = []\n",
    "        for q in insert_qs + [_q for _q in preds if _q not in insert_qs]:\n",
    "            new_preds.append((q, 1, cneed))  # qid, is_bm25, cneed_label\n",
    "        for q in notin_train:\n",
    "             if q not in preds:\n",
    "                new_preds.append((q, 0, cneed))  # qid, is_bm25, cneed_label\n",
    "        if topn > 0:\n",
    "            all_preds[tid] = new_preds[:topn]\n",
    "        else:\n",
    "            all_preds[tid] = new_preds\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_recall_results_with_score(dev, ir_id_columns='topic_id', ir_columns='initial_request', topn=-1):\n",
    "    all_preds = {}\n",
    "    for tid in dev[ir_id_columns].unique():\n",
    "        query = dev.loc[dev[ir_id_columns]==tid, ir_columns].tolist()[0]\n",
    "        if 'clarification_need' in dev.columns:\n",
    "            cneed = dev.loc[dev[ir_id_columns]==tid, 'clarification_need'].tolist()[0]\n",
    "        else:\n",
    "            cneed = 0\n",
    "#         bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), bm25_corpus, n=100)\n",
    "        bm25_scores = bm25.get_scores(stem_tokenize(query, True))\n",
    "        bm25_ranked_idxs = np.argsort(bm25_scores)[::-1]\n",
    "        if topn > 0:\n",
    "            bm25_ranked_idxs = bm25_ranked_idxs[:100]\n",
    "        bm25_ranked_list = [bm25_corpus[_i] for _i in bm25_ranked_idxs]\n",
    "        bm25_ranked_scores = [bm25_scores[_i] for _i in bm25_ranked_idxs]\n",
    "        bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
    "        preds = question_bank.set_index('all_token_str').loc[bm25_q_list, 'question_id'].tolist()\n",
    "        \n",
    "        bm25_ranked_list_onlyq = bm25_onlyq.get_top_n(stem_tokenize(query, True), bm25_corpus_onlyq, n=100)\n",
    "        bm25_q_list_onlyq = [' '.join(sent) for sent in bm25_ranked_list_onlyq]\n",
    "        preds_onlyq = question_bank.set_index('tokenized_question_str').loc[bm25_q_list_onlyq, 'question_id'].tolist()\n",
    "        \n",
    "        insert_qs = [(_q, _s) for _q, _s in zip(preds, bm25_ranked_scores) if _q in preds_onlyq]\n",
    "        other_qs = [(_q, _s) for _q, _s in  zip(preds, bm25_ranked_scores) if _q not in preds_onlyq]\n",
    "        new_preds = []\n",
    "        for q in insert_qs + other_qs:\n",
    "            new_preds.append((q[0], q[1], 1, cneed))  # qid, is_bm25, cneed_label\n",
    "        for q in notin_train:\n",
    "             if q not in preds:\n",
    "                new_preds.append((q, 0., 0, cneed))  # qid, is_bm25, cneed_label\n",
    "        if topn > 0:\n",
    "            all_preds[tid] = new_preds[:topn]\n",
    "        else:\n",
    "            all_preds[tid] = new_preds\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('../data/dev.tsv', sep='\\t')\n",
    "all_preds = get_bm25_recall_results_with_score(dev, 'topic_id', 'initial_request', topn=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197050"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('../data/dev.tsv', sep='\\t')\n",
    "\n",
    "run_file_path = './dev_bm25_added_normal'\n",
    "with open(run_file_path, 'w') as fo:\n",
    "    all_preds = get_bm25_recall_results_with_score(dev, topn=2000)\n",
    "    for tid, new_preds in all_preds.items():\n",
    "        for i, qinfos in enumerate(new_preds):\n",
    "            fo.write('{} 0 {} {} {} bm25\\n'.format(tid, qinfos[0], i, len(new_preds)-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall5: 0.34356393718081024\r\n",
      "Recall10: 0.6078327634697603\r\n",
      "Recall20: 0.6943680581544358\r\n",
      "Recall30: 0.7056124020458385\r\n",
      "Recall50: 0.7166712255752503\r\n",
      "Recall100: 0.7180045589085837\r\n",
      "Recall200: 0.7902837041699271\r\n",
      "Recall300: 0.8044615242858277\r\n",
      "Recall400: 0.8239103499699475\r\n",
      "Recall500: 0.836058809353701\r\n",
      "Recall600: 0.8482471847038411\r\n",
      "Recall700: 0.8564838255140113\r\n",
      "Recall800: 0.8709558143095295\r\n",
      "Recall900: 0.8812092079746878\r\n",
      "Recall1000: 0.897132179997505\r\n",
      "Recall1100: 0.8999893228546479\r\n",
      "Recall1200: 0.9086854012860205\r\n",
      "Recall1300: 0.9137576407081051\r\n",
      "Recall1400: 0.9207912541534833\r\n",
      "Recall1500: 0.9281868585490877\r\n",
      "Recall1600: 0.9355586534208825\r\n",
      "Recall1700: 0.9408829936833033\r\n",
      "Recall1800: 0.9423115651118747\r\n",
      "Recall1900: 0.9450734698737795\r\n",
      "Recall2000: 0.9502499404620147\r\n"
     ]
    }
   ],
   "source": [
    "! python ../official_src/clariq_eval_tool.py    --eval_task question_relevance\\\n",
    "                                --data_dir ../data/ \\\n",
    "                                --experiment_type dev \\\n",
    "                                --run_file {run_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG1: 0.18802083333333333\r\n",
      "NDCG3: 0.1684093677732618\r\n",
      "NDCG5: 0.16621118253836253\r\n",
      "NDCG10: 0.1506512398424613\r\n",
      "NDCG20: 0.13655006163151873\r\n",
      "P1: 0.23125\r\n",
      "P3: 0.20416666666666666\r\n",
      "P5: 0.19375\r\n",
      "P10: 0.15687500000000001\r\n",
      "P20: 0.1225\r\n",
      "MRR100: 0.317345372146148\r\n"
     ]
    }
   ],
   "source": [
    "! python ../official_src/clariq_eval_tool.py    --eval_task document_relevance\\\n",
    "                                --data_dir ../data/ \\\n",
    "                                --experiment_type dev \\\n",
    "                                --run_file {run_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qids = question_bank['question_id'].values\n",
    "def get_random_sample(dev, ir_id_columns='topic_id', ir_columns='initial_request', topn=500):\n",
    "    all_preds = {}\n",
    "    for tid in tqdm(dev[ir_id_columns].unique()):\n",
    "        all_preds[tid] = []\n",
    "        random_samples = np.random.choice(all_qids, topn, replace=False)\n",
    "        for qid in random_samples:\n",
    "            all_preds[tid].append((qid,))\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset, ir_id_columns='topic_id', ir_columns='initial_request', topn=500, is_test=False):\n",
    "    tid2qids = get_bm25_recall_results_with_score(dataset, ir_id_columns, ir_columns, topn=topn)\n",
    "#     tid2qids = get_random_sample(dataset, ir_id_columns, ir_columns, topn)\n",
    "    if is_test:\n",
    "        new_tid2qids = {}\n",
    "        for tid in tqdm(tid2qids):\n",
    "            new_tid2qids[tid] = []\n",
    "            for qinfo in tid2qids[tid]:\n",
    "                new_qinfo = qinfo + (0,)   # qid, is_bm25, bm25_score, cneed, is_next\n",
    "                new_tid2qids[tid].append(new_qinfo)\n",
    "        return new_tid2qids\n",
    "    else:\n",
    "        new_tid2qids = {}\n",
    "        for tid in tqdm(tid2qids):\n",
    "            pos_qids = dataset[dataset[ir_id_columns] == tid]['question_id'].unique().tolist()\n",
    "            cneed = dataset[dataset[ir_id_columns] == tid]['clarification_need'].values[0]\n",
    "            pos_qids = set(pos_qids)\n",
    "            new_tid2qids[tid] = []\n",
    "            for qid in pos_qids:\n",
    "                new_qinfo = (qid, False, 0., cneed, 1)\n",
    "#                 new_qinfo = (qid, 1)\n",
    "                new_tid2qids[tid].append(new_qinfo)\n",
    "            for qinfo in tid2qids[tid]:\n",
    "                if qinfo[0] not in pos_qids:\n",
    "                    new_qinfo = qinfo + (0,)   # qid, is_bm25, bm25_score, cneed, is_next\n",
    "                    new_tid2qids[tid].append(new_qinfo)\n",
    "        return new_tid2qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(tid2qids, dataset, question_bank, ir_id_columns='topic_id', ir_columns='initial_request', is_test=False):\n",
    "    tmp_df = {'tid': [], 'irid': [], 'initial_request': [], 'qid': [], 'question': [], 'label': [], 'is_bm25': [], 'bm25_score': [], 'cneed': []}\n",
    "#     tmp_df = {'tid': [], 'initial_request': [], 'qid': [], 'question': [], 'label': []}\n",
    "    for tid, irid, query in tqdm(dataset[['topic_id', ir_id_columns, ir_columns]].drop_duplicates().values):\n",
    "        qinfos = tid2qids[irid]\n",
    "        \n",
    "        for qinfo in qinfos:\n",
    "            qid = qinfo[0]\n",
    "            question = question_bank[question_bank['question_id'] == qid]['question'].values[0]\n",
    "            tmp_df['tid'].append(tid)\n",
    "            tmp_df['qid'].append(qid)\n",
    "            tmp_df['irid'].append(irid)\n",
    "            tmp_df['initial_request'].append(query)\n",
    "            tmp_df['question'].append(question)\n",
    "            tmp_df['label'].append(qinfo[-1])\n",
    "            tmp_df['bm25_score'].append(qinfo[1])\n",
    "            tmp_df['is_bm25'].append(qinfo[2])\n",
    "            tmp_df['cneed'].append(qinfo[3])\n",
    "            \n",
    "    tmp_df = pd.DataFrame(tmp_df)\n",
    "    return tmp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bm25 to sample neg_data\n",
    "trainset = pd.read_csv('../data/train.tsv', sep='\\t')\n",
    "devset = pd.read_csv('../data/dev.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:00<00:00, 327.39it/s]\n",
      "100%|██████████| 187/187 [01:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93657"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'topic_id'\n",
    "ir_columns='initial_request'\n",
    "train_tid2qids = build_dataset(trainset, ir_id_columns, ir_columns, topn=500)\n",
    "traindf1 = format_dataset(train_tid2qids, trainset, question_bank, ir_id_columns, ir_columns)\n",
    "len(traindf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf1.to_csv('../data/pair_datas_src/pair_trainset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:00<00:00, 480.60it/s]\n",
      "100%|██████████| 187/187 [00:31<00:00,  5.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37560"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'topic_id'\n",
    "ir_columns='topic_desc'\n",
    "train_tid2qids2 = build_dataset(trainset, ir_id_columns, ir_columns, topn=200)\n",
    "traindf2 = format_dataset(train_tid2qids2, trainset, question_bank, ir_id_columns, ir_columns)\n",
    "len(traindf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [00:01<00:00, 395.81it/s]\n",
      "100%|██████████| 638/638 [01:48<00:00,  5.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128597"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'facet_id'\n",
    "ir_columns='facet_desc'\n",
    "train_tid2qids3 = build_dataset(trainset, ir_id_columns, ir_columns, topn=200)\n",
    "traindf3 = format_dataset(train_tid2qids3, trainset, question_bank, ir_id_columns, ir_columns)\n",
    "len(traindf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259814"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.concat([traindf1, traindf2, traindf3])\n",
    "len(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 17588/68556 [00:40<01:35, 534.75it/s]"
     ]
    }
   ],
   "source": [
    "traindf.to_csv('../data/pair_datas/pair_trainset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 365.97it/s]\n",
      "100%|██████████| 50/50 [00:20<00:00,  2.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25107"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'topic_id'\n",
    "ir_columns='initial_request'\n",
    "dev_tid2qids = build_dataset(devset, ir_id_columns, ir_columns, topn=500)\n",
    "devdf1 = format_dataset(dev_tid2qids, devset, question_bank, ir_id_columns, ir_columns)\n",
    "len(devdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "devdf1.to_csv('../data/pair_datas_src/pair_devset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 357.68it/s]\n",
      "100%|██████████| 50/50 [00:08<00:00,  5.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10160"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'topic_id'\n",
    "ir_columns='topic_desc'\n",
    "dev_tid2qids2 = build_dataset(devset, ir_id_columns, ir_columns, topn=200)\n",
    "devdf2 = format_dataset(dev_tid2qids2, devset, question_bank, ir_id_columns, ir_columns)\n",
    "len(devdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 365.84it/s]\n",
      "100%|██████████| 163/163 [00:27<00:00,  5.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33289"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_id_columns = 'facet_id'\n",
    "ir_columns='facet_desc'\n",
    "dev_tid2qids3 = build_dataset(devset, ir_id_columns, ir_columns, topn=200)\n",
    "devdf3 = format_dataset(dev_tid2qids3, devset, question_bank, ir_id_columns, ir_columns)\n",
    "len(devdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68556"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devdf = pd.concat([devdf1, devdf2, devdf3])\n",
    "len(devdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "devdf.to_csv('../data/pair_datas/pair_devset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic_id', 'initial_request', 'topic_desc', 'clarification_need',\n",
      "       'facet_id', 'facet_desc', 'question_id', 'question', 'answer'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 293.73it/s]\n",
      "100%|██████████| 50/50 [01:14<00:00,  1.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100036"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv('../data/dev.tsv', sep='\\t')\n",
    "# testset.rename(columns={'initial request': 'initial_request'}, inplace=True)\n",
    "print(testset.columns)\n",
    "test_tid2qids = build_dataset(testset, ir_id_columns='topic_id', ir_columns='initial_request', topn=2000, is_test=False)\n",
    "test_df = format_dataset(test_tid2qids, testset, question_bank, ir_id_columns='topic_id', ir_columns='initial_request', is_test=False)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/pair_datas/pair_devset_for_test_new.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic_id', 'initial_request'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:00<00:00, 895.04it/s]\n",
      "100%|██████████| 61/61 [01:35<00:00,  1.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv('../data/test.tsv', sep='\\t')\n",
    "testset.rename(columns={'initial request': 'initial_request'}, inplace=True)\n",
    "print(testset.columns)\n",
    "test_tid2qids = build_dataset(testset, ir_id_columns='topic_id', ir_columns='initial_request', topn=2000, is_test=True)\n",
    "test_df = format_dataset(test_tid2qids, testset, question_bank, ir_id_columns='topic_id', ir_columns='initial_request', is_test=True)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/pair_datas/pair_testset_new.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240401"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv('../data/test.tsv', sep='\\t')\n",
    "testset.rename(columns={'initial request': 'initial_request'}, inplace=True)\n",
    "test_tid2qids = {}\n",
    "for tid in testset['topic_id'].values:\n",
    "    test_tid2qids[tid] = set(question_bank['question_id'].values.tolist())\n",
    "test_df = format_dataset(test_tid2qids, testset, question_bank, is_test=True)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add document relevent scores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_dict(eval_file_path, topic_file_path):\n",
    "    topic_df = pd.read_csv(topic_file_path, sep='\\t')\n",
    "    facet_array = topic_df['facet_id'].values\n",
    "    eval_dict = pickle.load(open(eval_file_path, 'rb'))\n",
    "    # we keep only the instances in the topic file.\n",
    "    new_eval_dict = {}\n",
    "    for metric in eval_dict:\n",
    "        new_eval_dict[metric] = {}\n",
    "        for fid in eval_dict[metric]:\n",
    "            if fid in facet_array:\n",
    "                new_eval_dict[metric][fid] = eval_dict[metric][fid]\n",
    "    return new_eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "topic_file_path = '../data/train.tsv'\n",
    "eval_file_path = '../data/single_turn_train_eval.pkl'\n",
    "new_train_eval_dict = load_eval_dict(eval_file_path, topic_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "topic_file_path = '../data/dev.tsv'\n",
    "eval_file_path = '../data/single_turn_train_eval.pkl'\n",
    "new_dev_eval_dict = load_eval_dict(eval_file_path, topic_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dr_score(dev_df, devset, new_eval_dict, dr_key='MRR100'):\n",
    "#     dev_df = pd.read_csv(topic_file_path, sep='\\t')\n",
    "#     dev_tid2fid = dev_df.set_index('topic_id')['facet_id'].to_dict()\n",
    "\n",
    "#     devset = pd.read_csv(format_file_path, sep='\\t')\n",
    "    vals = []\n",
    "    for tid, qid in tqdm(devset[['tid', 'qid']].values):\n",
    "        ttt = (dev_df['topic_id'] == tid) & (dev_df['question_id'] == qid)\n",
    "        fids = dev_df[ttt]['facet_id'].values.tolist()\n",
    "        mrr = []\n",
    "        for fid in fids:\n",
    "            mrr.append(0.)\n",
    "            if fid in new_eval_dict[dr_key]:\n",
    "                if qid in new_eval_dict[dr_key][fid]:\n",
    "                    mrr[-1] = new_eval_dict[dr_key][fid][qid]['with_answer']\n",
    "        vals.append(np.mean(mrr) if len(mrr) > 0 else 0.)\n",
    "    devset[dr_key] = vals\n",
    "    return devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_score_tids(devset, dr_key='NDCG3'):\n",
    "    tid2maxscore = {}\n",
    "    for tid in tqdm(devset['tid'].unique()):\n",
    "        tmp = devset[devset['tid'] == tid][dr_key].values\n",
    "        maxscore = np.max(tmp)\n",
    "        tid2maxscore[tid] = maxscore\n",
    "    \n",
    "    is_max_drkey = []\n",
    "    for tid, qid, val in tqdm(devset[['tid', 'qid', dr_key]].values):\n",
    "        if val == tid2maxscore[tid] and val > 0:\n",
    "            is_max_drkey.append(1)\n",
    "        else:\n",
    "            is_max_drkey.append(0)\n",
    "    \n",
    "    devset['top1_label'] = is_max_drkey\n",
    "    return devset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25603/25603 [00:35<00:00, 725.74it/s]\n",
      "100%|██████████| 25603/25603 [00:32<00:00, 780.91it/s]\n",
      "100%|██████████| 25603/25603 [00:32<00:00, 784.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv('../data/dev.tsv', sep='\\t')\n",
    "devset = pd.read_csv('../data/pair_datas_src/pair_devset.tsv', sep='\\t')\n",
    "for dr_key in ['MRR100', 'P1', 'NDCG3']:\n",
    "    devset = add_dr_score(dev_df, devset, new_dev_eval_dict, dr_key=dr_key)\n",
    "devset.to_csv('../data/pair_datas_src/pair_devset_more_values.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 602.34it/s]\n",
      "100%|██████████| 25603/25603 [00:00<00:00, 292731.42it/s]\n"
     ]
    }
   ],
   "source": [
    "devset = pd.read_csv('../data/pair_datas_src/pair_devset_more_values.tsv', sep='\\t')\n",
    "devset = get_max_score_tids(devset)\n",
    "devset = devset.to_csv('../data/pair_datas_src/pair_devset_more_values.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95761/95761 [02:38<00:00, 603.41it/s]\n",
      "100%|██████████| 95761/95761 [02:40<00:00, 598.14it/s]\n",
      "100%|██████████| 95761/95761 [02:38<00:00, 604.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.tsv', sep='\\t')\n",
    "trainset = pd.read_csv('../data/pair_datas_src/pair_trainset.tsv', sep='\\t')\n",
    "for dr_key in ['MRR100', 'P1', 'NDCG3']:\n",
    "    trainset = add_dr_score(train_df, trainset, new_train_eval_dict, dr_key=dr_key)\n",
    "trainset.to_csv('../data/pair_datas_src/pair_trainset_more_values.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:00<00:00, 583.59it/s]\n",
      "100%|██████████| 95761/95761 [00:00<00:00, 428178.55it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv('../data/pair_datas_src/pair_trainset_more_values.tsv', sep='\\t')\n",
    "trainset = get_max_score_tids(trainset)\n",
    "trainset = trainset.to_csv('../data/pair_datas_src/pair_trainset_more_values.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_for_test = pd.read_csv('../data/pair_datas/pair_devset_for_test.tsv', sep='\\t')\n",
    "devset_for_test[dr_key] = 0.\n",
    "devset_for_test.to_csv('../data/pair_datas/pair_devset_for_test.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv('../data/pair_datas/pair_testset.tsv', sep='\\t')\n",
    "testset[dr_key] = 0.\n",
    "testset.to_csv('../data/pair_datas/pair_testset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2527"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(devset[devset['MRR100'] > 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_for_test = pd.read_csv('../data/pair_datas_src/pair_devset_for_test_all.tsv', sep='\\t')\n",
    "testset = pd.read_csv('../data/pair_datas_src/pair_testset_all.tsv', sep='\\t')\n",
    "for dr_key in ['MRR100', 'P1', 'NDCG3', 'top1_label']:\n",
    "    devset_for_test[dr_key] = 0.\n",
    "    testset[dr_key] = 0.\n",
    "devset_for_test.to_csv('../data/pair_datas_src/pair_devset_for_test_all.tsv', sep='\\t', index=None)\n",
    "testset.to_csv('../data/pair_datas_src/pair_testset_all.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv('../data/pair_datas_src/pair_trainset_more_values.tsv', sep='\\t')\n",
    "devset = pd.read_csv('../data/pair_datas_src/pair_devset_more_values.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tid2pos = {}\n",
    "tid2neg = {}\n",
    "for tid, p1, p2, label, label2 in trainset[['tid', 'initial_request', 'question', 'label', 'top1_label']].values:\n",
    "    if tid not in tid2pos:\n",
    "        tid2pos[tid] = []\n",
    "    if tid not in tid2neg:\n",
    "        tid2neg[tid] = []\n",
    "    if label2 == 0:\n",
    "        tid2neg[tid].append((p1, p2, label2))\n",
    "    elif label2 == 1:\n",
    "        tid2pos[tid].append((p1, p2, label2))\n",
    "\n",
    "all_datas = []\n",
    "for tid in trainset['tid'].unique():\n",
    "    pos = tid2pos[tid]\n",
    "    neg = tid2neg[tid][:len(pos)*5]\n",
    "    for p1, p2, label2 in pos + neg:\n",
    "        p1 = p1.lower()\n",
    "        p2 = p2.lower()\n",
    "        data = {'tid': tid, 'p1': p1, 'p2': p2}\n",
    "        data['label'] = label2\n",
    "        all_datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 278)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_datas), len([_d for _d in all_datas if _d['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build Clarification Need Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_need_data(trainset, is_test=False):\n",
    "    if is_test:\n",
    "        cneed_trainset = trainset[['topic_id', 'initial_request']].drop_duplicates().reset_index(drop=True)\n",
    "        cneed_trainset['label'] = 0\n",
    "    else:\n",
    "        cneed_trainset = trainset[['topic_id', 'initial_request', 'clarification_need']].drop_duplicates().reset_index(drop=True)\n",
    "        cneed_trainset.rename(columns={'clarification_need': 'label'}, inplace=True)\n",
    "#     has_noq = []\n",
    "#     for tid in cneed_trainset['topic_id']:\n",
    "#         qids = trainset[trainset['topic_id'] == tid]['question']\n",
    "#         noq = qids.isnull().values.any()\n",
    "#         has_noq.append(noq)\n",
    "#     cneed_trainset['has_noq'] = has_noq\n",
    "    return cneed_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cneed_trainset = build_need_data(trainset)\n",
    "cneed_trainset.to_csv('../data/sent_datas/sent_trainset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cneed_devset = build_need_data(devset)\n",
    "cneed_devset.to_csv('../data/sent_datas/sent_devset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cneed_testset = build_need_data(testset, is_test=True)\n",
    "testset.rename(columns={'initial request': 'initial_request'}, inplace=True)\n",
    "cneed_testset.to_csv('../data/sent_datas/sent_testset.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>label</th>\n",
       "      <th>rlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>251.983607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.868852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.060767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.514237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>201.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>225.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>276.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id  label       rlen\n",
       "count   61.000000   61.0  61.000000\n",
       "mean   251.983607    0.0  34.868852\n",
       "std     30.060767    0.0  10.514237\n",
       "min    201.000000    0.0   8.000000\n",
       "25%    225.000000    0.0  28.000000\n",
       "50%    255.000000    0.0  34.000000\n",
       "75%    276.000000    0.0  42.000000\n",
       "max    300.000000    0.0  67.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cneed_testset['rlen'] = cneed_testset['initial_request'].apply(len)\n",
    "cneed_testset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>initial_request</th>\n",
       "      <th>rlen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_id  initial_request  rlen\n",
       "label                                 \n",
       "1             4                4     4\n",
       "2            21               21    21\n",
       "3            16               16    16\n",
       "4             9                9     9"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cneed_devset.groupby(by=['label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('../data/sent_datas/sent_trainset.tsv', sep='\\t')\n",
    "devdf = pd.read_csv('../data/sent_datas/sent_devset.tsv', sep='\\t')\n",
    "alldf = pd.concat([traindf, devdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>initial_request</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>What is Fickle Creek Farm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105</td>\n",
       "      <td>Tell me about sonoma county medical services.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>Tell me about of Ralph Owen Brester.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>I'm looking for information about mayo clinic ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                    initial_request  label\n",
       "0         1                   Tell me about Obama family tree.      2\n",
       "1       102                          What is Fickle Creek Farm      2\n",
       "2       105      Tell me about sonoma county medical services.      2\n",
       "3       108               Tell me about of Ralph Owen Brester.      1\n",
       "4       109  I'm looking for information about mayo clinic ...      2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = alldf[['topic_id', 'label']].groupby(by='label').count().values.reshape(-1)\n",
    "cnt = np.concatenate([[0], cnt])\n",
    "cntsum = np.sum(cnt)\n",
    "prior = cnt / cntsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.12236287, 0.40084388, 0.32911392, 0.14767932])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
